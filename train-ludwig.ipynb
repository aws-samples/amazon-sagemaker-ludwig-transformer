{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ludwig Transformer\n",
    "\n",
    "In this notebook we will use [ludwig](https://ludwig-ai.github.io/ludwig-docs/) to create a deep learning model built on tensorflow 2.3 for classifiying reviews.\n",
    "\n",
    "This notebook takes you through building custom containers for training and deployment in Amazon SageMaker.  \n",
    "* Please add managed policy `AmazonEC2ContainerRegistryFullAccess` to your notebook role to publish to ECR.\n",
    "\n",
    "One you have built the base container, you can jump to directly update the:\n",
    "* [Model Container](#Model-container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker seaborn\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Download a sample form the [Amazon Customer Reviews](https://s3.amazonaws.com/amazon-reviews-pds/readme.html) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz reviews.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the compressed reviews into pandas, selecting the review headling, body and star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a sample of the rows\n",
    "df_reviews = pd.read_csv('reviews.tsv.gz', compression='gzip', error_bad_lines=False,  nrows=15000,\n",
    "                         sep='\\t', usecols=['review_headline', 'review_body', 'star_rating']).dropna()\n",
    "df_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution between of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(df_reviews, x='star_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train (90%) / validation (5%) / test (5%) and save to file local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df_reviews, test_size=0.1, random_state=42) \n",
    "val_df, test_df = train_test_split(val_df, test_size=0.5, random_state=42)\n",
    "print('split train: {}, val: {}, test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv', index=False, header=True)\n",
    "val_df.to_csv('validation.csv', index=False, header=True)\n",
    "test_df.to_csv('test.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Push containers\n",
    "\n",
    "Define the shell script to build and push images to ECR registry (requires IAM permissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build_and_push.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "image=$1\n",
    "dockerfile=${2:-Dockerfile}\n",
    "inputdir=${3:-container}\n",
    "\n",
    "if [ \"$image\" == \"\" ]\n",
    "then\n",
    "    echo \"Usage: $0 <image-name> [dockerfile]\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    exit 255\n",
    "fi\n",
    "\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-ap-southeast-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${image}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${image}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region \"${region}\" | docker login --username AWS --password-stdin \"${account}\".dkr.ecr.\"${region}\".amazonaws.com\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${image} -f ${inputdir}/${dockerfile} ${inputdir}\n",
    "docker tag ${image} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training base container \n",
    "\n",
    "Write local files for Docker training and inference under a new `container` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile-training\n",
    "#\n",
    "# Ludwig Docker image with full set of pre-requiste packages to support these capabilities\n",
    "#   text features\n",
    "#   image features\n",
    "#   audio features\n",
    "#   visualizations\n",
    "#   hyperparameter optimization\n",
    "#   distributed training\n",
    "#   model serving\n",
    "#\n",
    "FROM tensorflow/tensorflow:2.3.0-gpu\n",
    "\n",
    "# Set a docker label to advertise multi-model support on the container\n",
    "LABEL com.amazonaws.sagemaker.capabilities.multi-models=false\n",
    "# Set a docker label to enable container to use SAGEMAKER_BIND_TO_PORT environment variable if present\n",
    "LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true    \n",
    "    \n",
    "RUN apt-get -y update && apt-get -y install \\\n",
    "    git \\\n",
    "    libsndfile1 \\\n",
    "    cmake \\\n",
    "    libcudnn7=7.6.5.32-1+cuda10.1 \\\n",
    "    libnccl2=2.7.8-1+cuda10.1 \\\n",
    "    libnccl-dev=2.7.8-1+cuda10.1\n",
    "\n",
    "RUN git clone --depth=1 https://github.com/uber/ludwig.git \\\n",
    "    && cd ludwig/ \\\n",
    "    && HOROVOD_GPU_OPERATIONS=NCCL \\\n",
    "       HOROVOD_WITH_TENSORFLOW=1 \\\n",
    "       HOROVOD_WITHOUT_MPI=1 \\\n",
    "       HOROVOD_WITHOUT_PYTORCH=1 \\\n",
    "       HOROVOD_WITHOUT_MXNET=1 \\\n",
    "    && pip install --no-cache-dir '.[full]'\n",
    "\n",
    "# install the SageMaker Inference & Training Toolkits\n",
    "RUN pip install --upgrade pip\n",
    "RUN pip --no-cache-dir install sagemaker-training\n",
    "\n",
    "# add custom transformer directory\n",
    "COPY transformer /opt/ml/code/transformer\n",
    "\n",
    "# set the working directory to be where code is\n",
    "RUN mkdir -p /opt/ml/code\n",
    "WORKDIR /opt/ml/code\n",
    "\n",
    "# copy the training script and configuration\n",
    "COPY train.py /opt/ml/code/train.py\n",
    "\n",
    "# define train.py as the script entry point\n",
    "ENV SAGEMAKER_PROGRAM train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a HuggingFace [Tensorflow pre-trained model](https://huggingface.co/models?filter=tf) to be included as part of our container so we do not have to download this on-demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync s3://models.huggingface.co/bert/gilf/english-yelp-sentiment ./container/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/train.py\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_csv(input_dir):\n",
    "    # Take the set of 1 or more files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('csv') ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(f'No csv files found in {input_dir}')\n",
    "    df = pd.concat([pd.read_csv(file) for file in input_files])\n",
    "    print(f'Loaded {len(input_files)} files from {input_dir}, shape: {df.shape}')\n",
    "    return df\n",
    "    \n",
    "def train(args):\n",
    "    # output directories\n",
    "    print(f'input train: {args.training_dir}, val: {args.validation_dir}, test: {args.testing_dir}')\n",
    "    print(f'output model: {args.model_dir}, data: {args.output_data_dir}')\n",
    "\n",
    "    # configure integrations https://ludwig-ai.github.io/ludwig-docs/user_guide/#integrations\n",
    "    try:\n",
    "        import ludwig.contrib\n",
    "        if args.integration == 'comet':\n",
    "            ludwig.contrib.use_contrib('comet')\n",
    "            print(f'using comet integration')\n",
    "        elif args.integration == 'wandb':\n",
    "            ludwig.contrib.use_contrib('wandb')\n",
    "            print(f'using wandb integration')\n",
    "    except Exception as e:\n",
    "        print('integration not supported: {}'.format(e))\n",
    "\n",
    "    # import ludwig after contrib incase we need to hook TF prior to loading\n",
    "    from ludwig.api import LudwigModel\n",
    "    \n",
    "    # loading csv dataframes\n",
    "    train_df = read_csv(args.training_dir)\n",
    "    val_df = read_csv(args.validation_dir)\n",
    "    test_df = read_csv(args.testing_dir)\n",
    "    \n",
    "    # train the model based on config yaml file\n",
    "    ludwig_model = LudwigModel('config.yml')\n",
    "    train_stats, _, _  = ludwig_model.train(\n",
    "        experiment_name=args.experiment_name,\n",
    "        model_name=args.model_name,\n",
    "        training_set=train_df,\n",
    "        validation_set=val_df,\n",
    "        test_set=test_df,\n",
    "        output_directory = args.output_data_dir, # Save experiment to output data dir\n",
    "        skip_save_training_statistics=False, # Save training results to file\n",
    "        skip_save_log=False, # Save tensorboard logs\n",
    "        skip_save_progress = True, # Don't save progress for now\n",
    "    )\n",
    "    \n",
    "    print('saving model')\n",
    "    \n",
    "    # Save the latest model to model_directory\n",
    "    ludwig_model.save(args.model_dir)\n",
    "    \n",
    "    # Save the compiled SavedModel to model directory\n",
    "    ludwig_model.save_savedmodel(args.model_dir)\n",
    "    \n",
    "    print('emmiting metrics')\n",
    "    \n",
    "    # enuemrate through the channels and output features to get metrics\n",
    "    for channel in train_stats:\n",
    "        for output in ludwig_model.config['output_features']:\n",
    "            for metric in train_stats['training'][output['name']]:\n",
    "                # get the metric from last epoch\n",
    "                value = train_stats['training'][output['name']][metric][-1]\n",
    "                print('{}_{}={};'.format(channel, metric, value))\n",
    "    \n",
    "\n",
    "    print('evaluating test dataset')\n",
    "        \n",
    "    # output evaluations based on test\n",
    "    ludwig_model.evaluate(test_df,\n",
    "        output_directory=args.output_data_dir,\n",
    "        skip_save_unprocessed_output=True, # Only save CSV values\n",
    "        skip_save_predictions=False, # Write predictions to file\n",
    "        skip_save_eval_stats=False, # Write evaluation stats to file\n",
    "        collect_predictions=True,\n",
    "        collect_overall_stats=True,\n",
    "    )\n",
    "    \n",
    "    # Return the model \n",
    "    return ludwig_model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # reads input channels training and testing from the environment variables\n",
    "    parser.add_argument(\"--training-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
    "    parser.add_argument(\"--validation-dir\", type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"])\n",
    "    parser.add_argument(\"--testing-dir\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--experiment-name\", type=str, default='api_experiment')\n",
    "    parser.add_argument(\"--model-name\", type=str, default='run')\n",
    "    parser.add_argument(\"--integration\", type=str, required=False)\n",
    "    args = parser.parse_args()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the base training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_name = 'ludwig-training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh build_and_push.sh $training_image_name Dockerfile-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Container\n",
    "\n",
    "Create the `ludwig-inference` dockerfile that inherts from `ludwig-training`, adding support for multi-model-server and Java depdendancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile-inference\n",
    "FROM ludwig-training:latest\n",
    "\n",
    "# Install necessary dependencies for MMS and SageMaker Inference Toolkit\n",
    "RUN apt-get update && \\\n",
    "    apt-get -y install --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    ca-certificates \\\n",
    "    openjdk-8-jdk-headless\n",
    "\n",
    "# install the SageMaker Inference & Training Toolkits\n",
    "RUN pip --no-cache-dir install multi-model-server retrying sagemaker-inference\n",
    "\n",
    "# copy the training script and configuration\n",
    "COPY model_handler.py /opt/ml/code/model_handler.py\n",
    "COPY entrypoint.py /opt/ml/code/entrypoint.py\n",
    "RUN chmod +x /opt/ml/code/entrypoint.py\n",
    "\n",
    "# Run the entrypoint \n",
    "ENTRYPOINT [\"python\", \"/opt/ml/code/entrypoint.py\"]\n",
    "\n",
    "# Define command to be passed to the entrypoint\n",
    "CMD [\"serve\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the [inference handler for tensorflow](https://github.com/aws/sagemaker-tensorflow-training-toolkit) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/model_handler.py\n",
    "\"\"\"\n",
    "ModelHandler defines an example model handler for load and inference requests for MXNet CPU models\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker_inference import content_types, environment, utils, logging\n",
    "\n",
    "logger = logging.get_logger()\n",
    "\n",
    "# TODO: Consider how to handle multi-model in future\n",
    "ENABLE_MULTI_MODEL = os.getenv(\"SAGEMAKER_MULTI_MODEL\", \"false\") == \"true\"\n",
    "\n",
    "class ModelHandler(object):\n",
    "    \"\"\"\n",
    "    Ludwig Model handler implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.environment = environment.Environment()\n",
    "        self.initialized = False\n",
    "        self.ludwig_model = None\n",
    "    \n",
    "    def initialize(self, context):\n",
    "        \"\"\"\n",
    "        Initialize model. This will be called during model loading time\n",
    "        :param context: Initial context contains model server system properties.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.initialized = True\n",
    "        properties = context.system_properties\n",
    "        logger.debug('properties: {}'.format(properties))\n",
    "        \n",
    "        # Contains the url parameter passed to the load request\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        # model_dir = os.path.join(model_dir, 'api_experiment_run', 'model')\n",
    "        \n",
    "        # Load the model from the default \n",
    "        logger.info('loading from model_dir: {}'.format(model_dir))\n",
    "        self.ludwig_model = LudwigModel.load(model_dir)\n",
    "\n",
    "    def preprocess(self, request, content_type):\n",
    "        \"\"\"\n",
    "        Transform raw input into model input data.\n",
    "        :param request: list of raw requests\n",
    "        :return: list of preprocessed model input data\n",
    "        \"\"\"\n",
    "        # Take the input data and read into pandas dataframe\n",
    "\n",
    "        logger.info('preprocess content type: {}'.format(content_type))\n",
    "        logger.debug('input', request) # TEMP\n",
    "        \n",
    "        if content_type == 'text/csv':\n",
    "            df_input = pd.read_csv(StringIO(request))\n",
    "        else:\n",
    "            df_input = pd.read_json(StringIO(request))            \n",
    "        \n",
    "        logger.info('input shape: {}'.format(df_input.shape))\n",
    "        return df_input\n",
    "\n",
    "    def inference(self, df_input):\n",
    "        \"\"\"\n",
    "        Internal inference methods\n",
    "        :param model_input: transformed model input data list\n",
    "        :return: list of inference output in NDArray\n",
    "        \"\"\"\n",
    "        # Perform inference on this dataframe\n",
    "        \n",
    "        (df_pred, output_directory) = self.ludwig_model.predict(df_input)\n",
    "        return df_pred\n",
    "\n",
    "    def postprocess(self, df_pred, accept):\n",
    "        \"\"\"\n",
    "        Return predict result in as list.\n",
    "        :param inference_output: list of inference output\n",
    "        :return: list of predict results\n",
    "        \"\"\"\n",
    "        # Return a dictionary output formatted as a list for each item\n",
    "        logger.info('postprocess accept: {}'.format(accept))\n",
    "\n",
    "        if accept == 'text/csv':\n",
    "            output = df_pred.to_csv(index=False)\n",
    "        else:\n",
    "            output = df_pred.to_json()\n",
    "        \n",
    "        logger.info('output length: {}'.format(len(output)))\n",
    "        return output\n",
    "        \n",
    "    def handle(self, data, context):\n",
    "        \"\"\"\n",
    "        Call preprocess, inference and post-process functions\n",
    "        :param data: input data\n",
    "        :param context: mms context\n",
    "        \"\"\"\n",
    "        # Return an array of the result\n",
    "        \n",
    "        # get input data\n",
    "        input_data = data[0].get(\"body\")\n",
    "\n",
    "        # Get the content and accept types\n",
    "        request_processor = context.request_processor[0]\n",
    "        request_property = request_processor.get_request_properties()\n",
    "        content_type = utils.retrieve_content_type_header(request_property)\n",
    "        accept = request_property.get(\"Accept\") or request_property.get(\"accept\")\n",
    "        if not accept or accept == content_types.ANY:\n",
    "            accept = self.environment.default_accept # Should default to JSON\n",
    "        if content_type in content_types.UTF8_TYPES:\n",
    "            input_data = input_data.decode(\"utf-8\")                \n",
    "        \n",
    "        # Process input to get output\n",
    "        model_input = self.preprocess(input_data, content_type)\n",
    "        model_predictions = self.inference(model_input)\n",
    "        model_result = self.postprocess(model_predictions, accept)\n",
    "        \n",
    "        # Set the response content type\n",
    "        context.set_response_content_type(0, accept)\n",
    "        return [model_result]\n",
    "\n",
    "_service = ModelHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    if not _service.initialized:\n",
    "        _service.initialize(context)\n",
    "\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    return _service.handle(data, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the entrypoint code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/entrypoint.py\n",
    "import subprocess\n",
    "import sys\n",
    "import shlex\n",
    "import os\n",
    "from retrying import retry\n",
    "from subprocess import CalledProcessError\n",
    "from sagemaker_inference import model_server\n",
    "\n",
    "def _retry_if_error(exception):\n",
    "    return isinstance(exception, CalledProcessError or OSError)\n",
    "\n",
    "@retry(stop_max_delay=1000 * 50, retry_on_exception=_retry_if_error)\n",
    "def _start_mms():\n",
    "    # by default the number of workers per model is 1, but we can configure it through the\n",
    "    # environment variable below if desired.\n",
    "    # os.environ['SAGEMAKER_MODEL_SERVER_WORKERS'] = '2'\n",
    "    model_server.start_model_server(handler_service='/opt/ml/code/model_handler.py:handle')\n",
    "\n",
    "def main():\n",
    "    if sys.argv[1] == 'serve':\n",
    "        _start_mms()\n",
    "    else:\n",
    "        subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n",
    "\n",
    "    # prevent docker exit\n",
    "    subprocess.call(['tail', '-f', '/dev/null'])\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the inference container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_name = 'ludwig-inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh build_and_push.sh $inference_image_name Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model container\n",
    "\n",
    "Create the `ludwig-reviews` dockerfile that inherts from `ludwig-training`, adding the configuration and optional tracking integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile-reviews\n",
    "FROM ludwig-training:latest\n",
    "\n",
    "COPY config.yml /opt/ml/code/config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify this cell to write [ludwig config](https://ludwig-ai.github.io/ludwig-docs/user_guide/#configuration).  See the text-features in the [user guide](https://ludwig-ai.github.io/ludwig-docs/user_guide/#text-features) for more options such as setting the encoder, or processing.\n",
    "\n",
    "Encoders that you might want to try include:\n",
    "\n",
    "* `bert`\n",
    "* `gpt2`\n",
    "* `xlnet`\n",
    "* `roberta`\n",
    "* `distilbert`\n",
    "\n",
    "Or else you can download any pre-trained model and include the `pretrained_model_name_or_path` in the configuration.\n",
    "\n",
    "\n",
    "ðŸ‘‡ðŸ‘‡ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile container/config.yml\n",
    "input_features:\n",
    "    -   name: review_headline\n",
    "        type: text\n",
    "        level: word\n",
    "        encoder: parallel_cnn # Simple encoder for title\n",
    "    -   name: review_body\n",
    "        type: text\n",
    "        level: word\n",
    "        encoder: bert\n",
    "        preprocessing:\n",
    "            char_sequence_length_limit: 1024\n",
    "            pretrained_tokenizer_fast: true\n",
    "        pretrained_model_name_or_path: /opt/ml/code/transformer\n",
    "output_features:\n",
    "    -   name: star_rating\n",
    "        type: category\n",
    "training:\n",
    "    epochs: 5\n",
    "    early_stop: 1\n",
    "    batch_size: 16 # OOM for bert if we don't keep this small\n",
    "    learning_rate: 0.001\n",
    "    decay: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_image_name = 'ludwig-reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh build_and_push.sh $model_image_name Dockerfile-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate\n",
    "\n",
    "We can now create a training job using based on our custom `ludwig-reviews` container and eveluate results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Experiment\n",
    "\n",
    "Define functions to create the sagemaker experiment for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from botocore import exceptions as botoexceptions\n",
    "\n",
    "def create_or_load_experiment(experiment_name, description='', tags=[]):\n",
    "    try:\n",
    "        return Experiment.create(\n",
    "            experiment_name=experiment_name, description=description, tags=tags\n",
    "        )\n",
    "    except botoexceptions.ClientError as err:\n",
    "        errcontent = err.response[\"Error\"]\n",
    "        if (\n",
    "            errcontent[\"Code\"] == \"ValidationException\"\n",
    "            and \"must be unique\" in errcontent[\"Message\"]\n",
    "        ):\n",
    "            print(f\"Loading Experiment '{experiment_name}'\")\n",
    "            return Experiment.load(experiment_name)\n",
    "        raise err\n",
    "\n",
    "\n",
    "def create_or_load_trial(experiment_name, trial_name, display_name='Training', tags=[]):\n",
    "    try:\n",
    "        Trial.create(experiment_name=experiment_name, trial_name=trial_name, tags=tags)\n",
    "    except botoexceptions.ClientError as err:\n",
    "        errcontent = err.response[\"Error\"]\n",
    "        if (\n",
    "            errcontent[\"Code\"] == \"ValidationException\"\n",
    "            and \"must be unique\" in errcontent[\"Message\"]\n",
    "        ):\n",
    "            print(f\"Loading Trial '{trial_name}'\")\n",
    "            return Trial.load(trial_name)\n",
    "        raise err\n",
    "\n",
    "# Get the session and default bucket\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the trial name to identify the dataset and training job.\n",
    "\n",
    "ðŸ‘‡ðŸ‘‡ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = 'ludwig-reviews'\n",
    "trial_name = 'bert-yelp' ## << Update this for new experiment trial\n",
    "create_or_load_experiment(experiment_name=exp_name)\n",
    "create_or_load_trial(experiment_name=exp_name, trial_name=trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data\n",
    "\n",
    "Upload data to s3 in bucket prefixed with experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data previx version\n",
    "prefix = f'ludwig-transformer/{exp_name}/{trial_name}'\n",
    "\n",
    "s3_train_uri = session.upload_data('train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = session.upload_data('validation.csv', bucket, prefix + '/data/validation')\n",
    "s3_test_uri = session.upload_data('test.csv', bucket, prefix + '/data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "Get the model container `image_uri` based on current region and account id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the region and account id\n",
    "def get_image_uri(image_name):\n",
    "    region = boto3.session.Session().region_name\n",
    "    sts = boto3.client('sts')\n",
    "    account_id = sts.get_caller_identity()[\"Account\"]\n",
    "    return f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{image_name}:latest\"\n",
    "    \n",
    "training_image_uri = get_image_uri(model_image_name)\n",
    "print(training_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training job in the cloud or with `local` mode to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_metric_definitions(channels, metrics):\n",
    "    for ch in channels:\n",
    "        for metric in metrics:\n",
    "            yield { \"Name\": \"{}_{}\".format(ch, metric), \"Regex\": \"{}_{}=(.*?);\".format(ch, metric) }\n",
    "\n",
    "# Get metric definitions\n",
    "metric_definitions = list(get_metric_definitions(\n",
    "    ['training', 'validation'], \n",
    "    ['loss', 'mean_squared_error', 'mean_absolute_error', 'r2']))\n",
    "\n",
    "instance_type = \"ml.p3.2xlarge\"\n",
    "# instance_type = \"local\" # << Uncomment to run local training\n",
    "\n",
    "# set hyperameters, \n",
    "hyperparameters = {\n",
    "    \"model-name\": exp_name,\n",
    "    \"experiment-name\": trial_name, \n",
    "    \"integration\": 'comet', # Specify integration for additional logging\n",
    "}\n",
    "\n",
    "# Set the base job name\n",
    "base_job_name = f\"{exp_name}-{trial_name}\"\n",
    "\n",
    "# Create the estimator\n",
    "ludwig = sagemaker.estimator.Estimator(\n",
    "    training_image_uri,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "\n",
    "# Specify the data source\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=s3_train_uri, content_type=\"text/csv\")\n",
    "s3_input_val = sagemaker.inputs.TrainingInput(s3_data=s3_val_uri, content_type=\"text/csv\")\n",
    "s3_input_test = sagemaker.inputs.TrainingInput(s3_data=s3_test_uri, content_type=\"text/csv\")\n",
    "data = {\"training\": s3_input_train, \"validation\": s3_input_val,\"testing\": s3_input_test}\n",
    "\n",
    "# Fit the model\n",
    "ludwig.fit(inputs=data, experiment_config={\n",
    "    \"ExperimentName\": hyperparameters['model-name'], # group by mode\n",
    "    \"TrialName\": hyperparameters['experiment-name'], # trail by experiment\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated Training Time / Price\n",
    "\n",
    "On 15k records\n",
    "\n",
    "* ml.p3.2xlarge: 24 mins (0.4 hr) = \\$ 1.53\n",
    "\n",
    "Optionally attach to an existing model below if you are restarting your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# ludwig = sagemaker.estimator.Estimator.attach('ludwig-reviews-bert-yelp-2020-11-18-04-06-18-496')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model and output artifacts\n",
    "\n",
    "Get the model artifaction s3 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = ludwig.model_data\n",
    "output_uri = model_uri.replace('model.tar.gz', 'output.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model artifacts and extract to `model` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf model && mkdir -p model\n",
    "!aws s3 cp $model_uri .\n",
    "!tar xvzf model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download output data and extract to `results` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf output && mkdir -p results\n",
    "!aws s3 cp $output_uri .\n",
    "!tar xvzf output.tar.gz -C ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "\n",
    "Load back the test set and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load test dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['score'] = test_df['star_rating']\n",
    "\n",
    "# Load predictions\n",
    "pred_df = pd.read_csv('./results/star_rating_predictions.csv', header=None, names=['score_predictions'])\n",
    "prob_df = pd.read_csv('./results/star_rating_probability.csv', header=None, names=['score_probabilty'])\n",
    "\n",
    "# Join with the test dataset\n",
    "pred_df = test_df.join(pred_df).join(prob_df)\n",
    "print(pred_df.shape)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cf_matrix = confusion_matrix(pred_df['score'], pred_df['score_predictions'])\n",
    "print(cf_matrix)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the confusion matrix as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "classes = ['1','2','3','4','5'] # Define classes as strings\n",
    "cf_percent = cf_matrix / cf_matrix.astype(np.float).sum(axis=1)\n",
    "sns.heatmap(cf_percent, annot=True, xticklabels=classes, yticklabels=classes, fmt=\".0%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(pred_df['score_predictions'], test_df['score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curve for each of the classes by loading back the probabilites matrix aligned to the score columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the score column order\n",
    "with open('./model/training_set_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "score_cols = metadata['star_rating']['idx2str']\n",
    "\n",
    "# Load the probabilities matrix first UNK col\n",
    "prob_df = pd.read_csv('./results/star_rating_probabilities.csv', header=None, names=score_cols).drop('<UNK>', axis=1)\n",
    "prob_df = prob_df[classes] # re-order cols\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Convert string classes to int for binarize of predictions\n",
    "n_classes = len(classes)\n",
    "y_test = label_binarize(test_df['score'], classes=[int(c) for c in classes])\n",
    "y_score = prob_df.values\n",
    "assert y_test.shape == y_score.shape\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "lw=2\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(classes[i], roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test locally\n",
    "\n",
    "Deploy the model locally using the built container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker run --rm --name \"ludwig-inference\" \\\n",
    "    -p 8080:8080 \\\n",
    "    -v \"$PWD/model:/opt/ml/model\" \\\n",
    "    -v \"$PWD:/opt/ml/input\" ludwig-inference:latest serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While the above cell is running, click here [TEST NOTEBOOK](test-ludwig.ipynb) to run some tests.\n",
    ">\n",
    "> After you finish the tests, press **STOP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy\n",
    "\n",
    "Create the model specifying the inference container, and then deploy endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image_uri = get_image_uri(inference_image_name)\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the endpoint passing the `inference_image_uri` which will be passed down to create model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictor = ludwig.deploy(initial_instance_count=1, \n",
    "                          instance_type='ml.g4dn.xlarge',\n",
    "                          image_uri=inference_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally attach a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.predictor import Predictor\n",
    "# predictor = Predictor('ludwig-reviews-bert-yelp-2020-11-17-23-48-01-555')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Load a sample dataset and predict against the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv('test.csv')\n",
    "payload = test_df.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Make a prediction against the payload\n",
    "predictor.serializer = CSVSerializer()\n",
    "predictor.deserializer = CSVDeserializer()\n",
    "result = predictor.predict(data=payload)\n",
    "\n",
    "# Load the result as array where first element is the column names\n",
    "pred_df = pd.DataFrame(result[1:], columns=result[0])\n",
    "print(pred_df.shape)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save results to file so for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('./results/score_predictions.csv', header=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Ludwig CLI\n",
    "\n",
    "If you have a powerful enough notebook instance, you can execute the train and evaluate directly in the notebook with the [ludwig CLI](https://ludwig-ai.github.io/ludwig-docs/user_guide/#command-line-interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall -y horovod # uninstall horovod to get ludwig to work correctly in notebook\n",
    "!pip install -U ludwig[text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we have transformers and installed as part of ludwig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ludwig\n",
    "import transformers\n",
    "print('lugwig: {}, transformers: {}'.format(ludwig.__version__, transformers.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Run training in the shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir -p /opt/ml/code\n",
    "!sudo chmod 777 /opt/ml/code\n",
    "!ln -s $(pwd)/container/transformer /opt/ml/code/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ludwig train --config_file container/config.yml --experiment_name=exp1 --training_set train.csv --validation_set validation.csv --test_set test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Evalate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ludwig evaluate --dataset test.csv --model_path results/exp1_run/model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
